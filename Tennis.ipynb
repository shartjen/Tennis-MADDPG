{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env_file_name = \"Tennis_Windows_x86_64/Tennis.exe\"\n",
    "# env = UnityEnvironment(file_name=env_file_name)\n",
    "env = UnityEnvironment(file_name=env_file_name,no_graphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      "states shape :  (2, 24)\n",
      "Both states look like :  [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.         -13.30557251  -3.          -0.           0.\n",
      "   13.66344166  12.          -0.           0.        ]\n",
      " [  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.         -12.93397141  -3.           0.           0.\n",
      "  -13.66344166  12.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print('states shape : ',states.shape)\n",
    "print('Both states look like : ',states)\n",
    "print(2*states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    total_scores = []\n",
    "    for i in range(100):                                        # play game for 5 episodes\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        t = 0\n",
    "        while True:\n",
    "            actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            # print('actions : ',actions)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            t += 1\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        print('Score (max over agents) from episode {}: {}, and {} steps taken'.format(i, np.max(scores),t))\n",
    "        print(scores)\n",
    "        total_scores.append(scores)\n",
    "    print('Average Random Score : ', np.mean(total_scores))\n",
    "        \n",
    "def plot_results(results):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    plt.ion()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(results.all_rewards)), [np.sum(ar) for ar in results.all_rewards])\n",
    "    plt.plot(np.arange(len(results.avg_rewards)), results.avg_rewards)\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(results.critic_loss)), results.critic_loss)\n",
    "    plt.ylabel('critic_losses')\n",
    "    plt.xlabel('Learn Step #')\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(results.actor_loss)), results.actor_loss)\n",
    "    plt.ylabel('actor_losses')\n",
    "    plt.xlabel('Learn Step #')\n",
    "    plt.show()\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "New Run :\n",
      "-------------------------------------\n",
      "Config Parameters    : \n",
      "gamma                : 0.99\n",
      "tau                  : 0.01\n",
      "action_size          : 2\n",
      "state_size           : 24\n",
      "hidden_size          : 256\n",
      "buffer_size          : 50000\n",
      "batch_size           : 256\n",
      "seed                 : 1\n",
      "max_episodes         : 1000\n",
      "dropout              : 0.01\n",
      "learn_every          : 1\n",
      "learn_num            : 2\n",
      "critic_learning_rate : 0.001\n",
      "actor_learning_rate  : 0.001\n",
      "noise_decay          : 0.999\n",
      "sigma                : 0.2\n",
      "num_agents           : 2\n",
      "env_file_name        : Tennis_Windows_x86_64/Tennis.exe\n",
      "load_model           : True\n",
      "save_model           : True\n",
      "train_mode           : True\n",
      "brain_name           : TennisBrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 0/1000   0% ETA:  --:--:-- |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory loaded with length :  46779\n",
      "Running on device :  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\programdata\\anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 with 15 steps || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.999 || 0.384 seconds, mem : 46794\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 3/1000   0% ETA:  0:06:59 |                                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Hit\n",
      "\u001b[42mEpisode 2 with 61 steps || Reward : [0.1  0.19] || avg reward :  0.063 || Noise  0.997 || 0.344 seconds, mem : 46893\n",
      "\u001b[0m\u001b[41mEpisode 4 with 29 steps || Reward : [0.   0.09] || avg reward :  0.056 || Noise  0.995 || 0.249 seconds, mem : 46945\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 7/1000   0% ETA:  0:05:37 |                                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Hit\n",
      "\u001b[42mEpisode 5 with 80 steps || Reward : [0.2  0.09] || avg reward :  0.080 || Noise  0.994 || 0.481 seconds, mem : 47025\n",
      "\u001b[0m\u001b[41mEpisode 7 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.073 || Noise  0.992 || 0.279 seconds, mem : 47072\n",
      "\u001b[0m\u001b[41mEpisode 8 with 30 steps || Reward : [0.   0.09] || avg reward :  0.074 || Noise  0.991 || 0.305 seconds, mem : 47102\n",
      "\u001b[0m\u001b[41mEpisode 9 with 29 steps || Reward : [0.   0.09] || avg reward :  0.076 || Noise  0.990 || 0.235 seconds, mem : 47131\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 11/1000   1% ETA:  0:05:10 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 10 with 31 steps || Reward : [0.   0.09] || avg reward :  0.077 || Noise  0.989 || 0.250 seconds, mem : 47162\n",
      "\u001b[0m\u001b[41mEpisode 11 with 30 steps || Reward : [0.   0.09] || avg reward :  0.078 || Noise  0.988 || 0.281 seconds, mem : 47192\n",
      "\u001b[0m\u001b[44mEpisode 12 with 53 steps || Reward : [0.1  0.09] || avg reward :  0.080 || Noise  0.987 || 0.350 seconds, mem : 47245\n",
      "\u001b[0m\u001b[41mEpisode 13 with 30 steps || Reward : [-0.01  0.1 ] || avg reward :  0.081 || Noise  0.986 || 0.248 seconds, mem : 47275\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 15/1000   1% ETA:  0:04:59 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 14 with 25 steps || Reward : [-0.01  0.1 ] || avg reward :  0.083 || Noise  0.985 || 0.218 seconds, mem : 47300\n",
      "\u001b[0mDouble Hit\n",
      "\u001b[42mEpisode 16 with 88 steps || Reward : [0.2  0.19] || avg reward :  0.085 || Noise  0.983 || 0.478 seconds, mem : 47402\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 18/1000   1% ETA:  0:05:12 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double Hit\n",
      "\u001b[42mEpisode 17 with 89 steps || Reward : [0.2  0.19] || avg reward :  0.091 || Noise  0.982 || 0.488 seconds, mem : 47491\n",
      "\u001b[0m\u001b[44mEpisode 18 with 52 steps || Reward : [0.09 0.1 ] || avg reward :  0.092 || Noise  0.981 || 0.342 seconds, mem : 47543\n",
      "\u001b[0mDouble Hit\n",
      "\u001b[42mEpisode 19 with 75 steps || Reward : [0.2  0.09] || avg reward :  0.097 || Noise  0.980 || 0.397 seconds, mem : 47618\n",
      "\u001b[0m\u001b[41mEpisode 20 with 24 steps || Reward : [-0.01  0.1 ] || avg reward :  0.097 || Noise  0.979 || 0.218 seconds, mem : 47642\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 22/1000   2% ETA:  0:05:08 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 21 with 27 steps || Reward : [-0.01  0.1 ] || avg reward :  0.097 || Noise  0.978 || 0.252 seconds, mem : 47669\n",
      "\u001b[0m\u001b[44mEpisode 22 with 80 steps || Reward : [0.1  0.09] || avg reward :  0.097 || Noise  0.977 || 0.425 seconds, mem : 47749\n",
      "\u001b[0m\u001b[41mEpisode 23 with 30 steps || Reward : [-0.01  0.1 ] || avg reward :  0.098 || Noise  0.976 || 0.224 seconds, mem : 47779\n",
      "\u001b[0m\u001b[41mEpisode 24 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.098 || Noise  0.975 || 0.332 seconds, mem : 47812\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 26/1000   2% ETA:  0:05:05 |\\                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 25 with 30 steps || Reward : [0.   0.09] || avg reward :  0.097 || Noise  0.974 || 0.231 seconds, mem : 47842\n",
      "\u001b[0m\u001b[41mEpisode 27 with 30 steps || Reward : [0.   0.09] || avg reward :  0.094 || Noise  0.972 || 0.223 seconds, mem : 47890\n",
      "\u001b[0m\u001b[41mEpisode 28 with 30 steps || Reward : [0.   0.09] || avg reward :  0.093 || Noise  0.971 || 0.192 seconds, mem : 47920\n",
      "\u001b[0m\u001b[41mEpisode 29 with 30 steps || Reward : [0.   0.09] || avg reward :  0.093 || Noise  0.970 || 0.178 seconds, mem : 47950\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 32/1000   3% ETA:  0:04:39 ||                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 31 with 31 steps || Reward : [-0.01  0.1 ] || avg reward :  0.091 || Noise  0.968 || 0.174 seconds, mem : 47995\n",
      "\u001b[0m\u001b[41mEpisode 32 with 31 steps || Reward : [0.   0.09] || avg reward :  0.091 || Noise  0.968 || 0.222 seconds, mem : 48026\n",
      "\u001b[0m\u001b[41mEpisode 33 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.091 || Noise  0.967 || 0.257 seconds, mem : 48059\n",
      "\u001b[0m\u001b[41mEpisode 34 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.091 || Noise  0.966 || 0.276 seconds, mem : 48092\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 37/1000   3% ETA:  0:04:31 |/                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 36 with 31 steps || Reward : [-0.01  0.1 ] || avg reward :  0.089 || Noise  0.964 || 0.234 seconds, mem : 48138\n",
      "\u001b[0m\u001b[41mEpisode 38 with 30 steps || Reward : [0.   0.09] || avg reward :  0.087 || Noise  0.962 || 0.303 seconds, mem : 48182\n",
      "\u001b[0m\u001b[41mEpisode 39 with 30 steps || Reward : [0.   0.09] || avg reward :  0.087 || Noise  0.961 || 0.232 seconds, mem : 48212\n",
      "\u001b[0m\u001b[41mEpisode 40 with 34 steps || Reward : [ 0.1  -0.01] || avg reward :  0.087 || Noise  0.960 || 0.250 seconds, mem : 48246\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 42/1000   4% ETA:  0:04:24 |-                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 41 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.087 || Noise  0.959 || 0.235 seconds, mem : 48278\n",
      "\u001b[0m\u001b[41mEpisode 42 with 30 steps || Reward : [0.   0.09] || avg reward :  0.087 || Noise  0.958 || 0.292 seconds, mem : 48308\n",
      "\u001b[0m\u001b[41mEpisode 43 with 30 steps || Reward : [0.   0.09] || avg reward :  0.088 || Noise  0.957 || 0.233 seconds, mem : 48338\n",
      "\u001b[0m\u001b[41mEpisode 45 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.086 || Noise  0.955 || 0.238 seconds, mem : 48392\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 47/1000   4% ETA:  0:04:20 |\\                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 46 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.086 || Noise  0.954 || 0.280 seconds, mem : 48424\n",
      "\u001b[0m\u001b[41mEpisode 47 with 31 steps || Reward : [0.   0.09] || avg reward :  0.086 || Noise  0.953 || 0.252 seconds, mem : 48455\n",
      "\u001b[0m\u001b[41mEpisode 48 with 49 steps || Reward : [-0.01  0.1 ] || avg reward :  0.087 || Noise  0.952 || 0.297 seconds, mem : 48504\n",
      "\u001b[0m\u001b[41mEpisode 49 with 34 steps || Reward : [ 0.1  -0.01] || avg reward :  0.087 || Noise  0.951 || 0.237 seconds, mem : 48538\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 51/1000   5% ETA:  0:04:19 |||                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 50 with 30 steps || Reward : [0.   0.09] || avg reward :  0.087 || Noise  0.950 || 0.273 seconds, mem : 48568\n",
      "\u001b[0m\u001b[41mEpisode 52 with 31 steps || Reward : [-0.01  0.1 ] || avg reward :  0.085 || Noise  0.948 || 0.241 seconds, mem : 48615\n",
      "\u001b[0m\u001b[41mEpisode 53 with 30 steps || Reward : [0.   0.09] || avg reward :  0.086 || Noise  0.947 || 0.225 seconds, mem : 48645\n",
      "\u001b[0m\u001b[41mEpisode 54 with 31 steps || Reward : [-0.01  0.1 ] || avg reward :  0.086 || Noise  0.946 || 0.275 seconds, mem : 48676\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 56/1000   5% ETA:  0:04:14 |//                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 55 with 31 steps || Reward : [-0.01  0.1 ] || avg reward :  0.086 || Noise  0.946 || 0.247 seconds, mem : 48707\n",
      "\u001b[0m\u001b[41mEpisode 56 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.086 || Noise  0.945 || 0.281 seconds, mem : 48740\n",
      "\u001b[0m\u001b[41mEpisode 57 with 31 steps || Reward : [0.   0.09] || avg reward :  0.086 || Noise  0.944 || 0.234 seconds, mem : 48771\n",
      "\u001b[0m\u001b[41mEpisode 58 with 31 steps || Reward : [-0.01  0.1 ] || avg reward :  0.087 || Noise  0.943 || 0.284 seconds, mem : 48802\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 60/1000   6% ETA:  0:04:12 |--                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 59 with 30 steps || Reward : [0.   0.09] || avg reward :  0.087 || Noise  0.942 || 0.234 seconds, mem : 48832\n",
      "\u001b[0m\u001b[41mEpisode 60 with 33 steps || Reward : [0.09 0.  ] || avg reward :  0.087 || Noise  0.941 || 0.253 seconds, mem : 48865\n",
      "\u001b[0m\u001b[41mEpisode 61 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.087 || Noise  0.940 || 0.240 seconds, mem : 48898\n",
      "\u001b[0m\u001b[41mEpisode 62 with 31 steps || Reward : [-0.01  0.1 ] || avg reward :  0.087 || Noise  0.939 || 0.275 seconds, mem : 48929\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 64/1000   6% ETA:  0:04:10 |\\\\                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 63 with 33 steps || Reward : [-0.01  0.1 ] || avg reward :  0.087 || Noise  0.938 || 0.242 seconds, mem : 48962\n",
      "\u001b[0m\u001b[41mEpisode 64 with 30 steps || Reward : [0.   0.09] || avg reward :  0.087 || Noise  0.937 || 0.238 seconds, mem : 48992\n",
      "\u001b[0m\u001b[41mEpisode 65 with 34 steps || Reward : [ 0.1  -0.01] || avg reward :  0.088 || Noise  0.936 || 0.239 seconds, mem : 49026\n",
      "\u001b[0m\u001b[41mEpisode 66 with 31 steps || Reward : [-0.01  0.1 ] || avg reward :  0.088 || Noise  0.935 || 0.281 seconds, mem : 49057\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 68/1000   6% ETA:  0:04:10 |||                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[44mEpisode 67 with 70 steps || Reward : [0.1  0.09] || avg reward :  0.088 || Noise  0.934 || 0.385 seconds, mem : 49127\n",
      "\u001b[0m\u001b[41mEpisode 68 with 34 steps || Reward : [ 0.1  -0.01] || avg reward :  0.088 || Noise  0.933 || 0.249 seconds, mem : 49161\n",
      "\u001b[0m\u001b[41mEpisode 69 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.088 || Noise  0.932 || 0.278 seconds, mem : 49194\n",
      "\u001b[0m\u001b[41mEpisode 70 with 30 steps || Reward : [0.   0.09] || avg reward :  0.088 || Noise  0.931 || 0.232 seconds, mem : 49224\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 72/1000   7% ETA:  0:04:08 |//                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 71 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.088 || Noise  0.930 || 0.247 seconds, mem : 49257\n",
      "\u001b[0m\u001b[41mEpisode 72 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.089 || Noise  0.930 || 0.251 seconds, mem : 49290\n",
      "\u001b[0m\u001b[41mEpisode 73 with 31 steps || Reward : [-0.01  0.1 ] || avg reward :  0.089 || Noise  0.929 || 0.270 seconds, mem : 49321\n",
      "\u001b[0m\u001b[41mEpisode 74 with 41 steps || Reward : [-0.01  0.1 ] || avg reward :  0.089 || Noise  0.928 || 0.279 seconds, mem : 49362\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 76/1000   7% ETA:  0:04:07 |---                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 75 with 32 steps || Reward : [0.   0.09] || avg reward :  0.089 || Noise  0.927 || 0.247 seconds, mem : 49394\n",
      "\u001b[0m\u001b[41mEpisode 76 with 31 steps || Reward : [0.   0.09] || avg reward :  0.089 || Noise  0.926 || 0.249 seconds, mem : 49425\n",
      "\u001b[0m\u001b[41mEpisode 78 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.088 || Noise  0.924 || 0.252 seconds, mem : 49470\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 80/1000   8% ETA:  0:04:05 |\\\\\\                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[44mEpisode 79 with 52 steps || Reward : [0.09 0.1 ] || avg reward :  0.088 || Noise  0.923 || 0.317 seconds, mem : 49522\n",
      "\u001b[0m\u001b[41mEpisode 80 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.088 || Noise  0.922 || 0.249 seconds, mem : 49554\n",
      "\u001b[0m\u001b[41mEpisode 81 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.088 || Noise  0.921 || 0.274 seconds, mem : 49586\n",
      "\u001b[0m\u001b[41mEpisode 82 with 33 steps || Reward : [-0.01  0.1 ] || avg reward :  0.089 || Noise  0.920 || 0.242 seconds, mem : 49619\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 84/1000   8% ETA:  0:04:04 ||||                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 83 with 32 steps || Reward : [-0.01  0.1 ] || avg reward :  0.089 || Noise  0.919 || 0.262 seconds, mem : 49651\n",
      "\u001b[0m\u001b[41mEpisode 84 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.089 || Noise  0.918 || 0.268 seconds, mem : 49683\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 90/1000   9% ETA:  0:03:58 |///                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 92 with 31 steps || Reward : [0.   0.09] || avg reward :  0.082 || Noise  0.911 || 0.232 seconds, mem : 49811\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 95/1000   9% ETA:  0:03:54 |---                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 93 with 32 steps || Reward : [0.   0.09] || avg reward :  0.082 || Noise  0.910 || 0.248 seconds, mem : 49843\n",
      "\u001b[0m\u001b[41mEpisode 96 with 31 steps || Reward : [0.   0.09] || avg reward :  0.081 || Noise  0.908 || 0.250 seconds, mem : 49903\n",
      "\u001b[0m\u001b[41mEpisode 97 with 32 steps || Reward : [0.   0.09] || avg reward :  0.081 || Noise  0.907 || 0.247 seconds, mem : 49935\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 100/1000  10% ETA:  0:03:50 |\\\\\\                                    | \r"
     ]
    }
   ],
   "source": [
    "from maddpg import maddpg\n",
    "import cProfile\n",
    "DoProfile = False\n",
    "\n",
    "config = {\n",
    "    'gamma'               : 0.99,\n",
    "    'tau'                 : 0.01,\n",
    "    'action_size'         : action_size,\n",
    "    'state_size'          : state_size,\n",
    "    'hidden_size'         : 256,\n",
    "    'buffer_size'         : 50000,\n",
    "    'batch_size'          : 256,\n",
    "    'seed'                : 40,\n",
    "    'max_episodes'        : 1000,\n",
    "    'dropout'             : 0.01,      # currently not active\n",
    "    'learn_every'         : 1,\n",
    "    'learn_num'           : 2,\n",
    "    'critic_learning_rate': 1e-3,\n",
    "    'actor_learning_rate' : 1e-3,\n",
    "    'noise_decay'         : 0.999,\n",
    "    'sigma'               : 0.2,\n",
    "    'num_agents'          : num_agents,\n",
    "    'env_file_name'       : env_file_name,\n",
    "    'load_model'          : True,\n",
    "    'save_model'          : True,\n",
    "    'train_mode'          : True,\n",
    "    'brain_name'          : brain_name}\n",
    "\n",
    "def print_config(config):\n",
    "    print('Config Parameters    : ')\n",
    "    for c,k in config.items():\n",
    "        print('{:20s} : {}'.format(c,k))\n",
    "\n",
    "config_list = []\n",
    "result_list = []\n",
    "var_range = []\n",
    "# batch = [512,1024]\n",
    "# nd = [0.999, 0.998]\n",
    "# for l in learn:\n",
    "    # for b in batch:\n",
    "        # var_range.append([l,b])\n",
    "        # for h in hidden:\n",
    "            # for n in nd:\n",
    "var_range = [0.05] #, 0.1, 0.15, 0.2, 0.4, 0.6] # , 0.999975] # , 0.6, 0.7, 0.8] #, 0.45, 0.5]\n",
    "# var_range = [0.9998, 0.9999, 0.99995] # , 0.0003, 0.0005, 0.001]# [0.2, 0.25, 0.3]\n",
    "selected_seeds = [1,2,3,4,5] # [31,36,43,44] # 24,26, 33] # [8,16] # [7,9,13,15]\n",
    "# [8,16] for learn2,sig6, hidden256, batch 256, ind\n",
    "# [41,43,48,50,54,55,57,62,64,67,77,86]\n",
    "# num_runs = 50\n",
    "for param in range(len(var_range)):\n",
    "    alt_config = config.copy()\n",
    "    # alt_config['sigma'] = var_range[param]\n",
    "    # alt_config['noise_decay'] = var_range[param]\n",
    "    # alt_config['noise_scale_trigger'] = var_range[param]\n",
    "    # alt_config['actor_learning_rate'] = var_range[param]\n",
    "    # alt_config['learn_every_low'] = var_range[param][0]\n",
    "    num_runs = len(selected_seeds)\n",
    "    for main in range(num_runs):#len(tau_range)):\n",
    "        print('-------------------------------------')\n",
    "        print('New Run :')\n",
    "        print('-------------------------------------')\n",
    "        # env = UnityEnvironment(file_name=env_file_name,no_graphics=True)\n",
    "        # brain_name = env.brain_names[0]\n",
    "        # brain = env.brains[brain_name]\n",
    "        # alt_config['seed'] += 1\n",
    "        alt_config['seed'] = selected_seeds[main]\n",
    "        print_config(alt_config)\n",
    "        config_list.append(alt_config.copy())\n",
    "        alt_config['brain_name'] = brain_name\n",
    "        agent = maddpg(env, alt_config)\n",
    "        if DoProfile:cProfile.run(\"results = agent.train()\",'PerfStats')\n",
    "        else:results = agent.train()\n",
    "        result_list.append(results)\n",
    "        # all_rewards,avg_rewards,critic_losses,actor_losses = agent.train()\n",
    "        print_config(alt_config)\n",
    "        plot_results(results)\n",
    "        \n",
    "print('-------------------------------------')\n",
    "print('-------------------------------------')\n",
    "print('Summary :')\n",
    "print('-------------------------------------')\n",
    "print('-------------------------------------')\n",
    "for param in range(len(var_range)):\n",
    "    for main in range(num_runs):\n",
    "        print_config(config_list[param*num_runs+main])\n",
    "        plot_results(result_list[param*num_runs+main])\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
